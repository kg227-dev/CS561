{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-11 23:21:03.145087: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import models\n",
    "import keras.layers as kl\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Reshape\n",
    "from keras.layers import Convolution2D, MaxPooling1D\n",
    "from keras.metrics import SparseCategoricalCrossentropy\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import History\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding(dna_sequence):\n",
    "    \"\"\"\n",
    "    Converts a DNA sequence to one-hot-encoding\n",
    "    :param dna_sequence: nucleotide sequence\n",
    "    :type dna_sequence: str\n",
    "    :return: lst of encoding for each nucleotide in the original DNA string\n",
    "    :rtype: lst of lst of ints\n",
    "    :author: Sydney Ballard\n",
    "    \"\"\"\n",
    "    # Define a dictionary for 1-hot encoding\n",
    "    nucleotide_dict = {'A': [1, 0, 0, 0], \n",
    "                       'C': [0, 1, 0, 0], \n",
    "                       'G': [0, 0, 1, 0], \n",
    "                       'T': [0, 0, 0, 1]}\n",
    "    \n",
    "    # Convert the DNA sequence to 1-hot encoding\n",
    "    one_hot_encoding = [nucleotide_dict[base] for base in dna_sequence]  \n",
    "\n",
    "    # Return list of one hot encodings\n",
    "    return one_hot_encoding\n",
    "\n",
    "def parse_sequence_activity_mpra(file_name):\n",
    "    # read the contents of the text file into a list of strings\n",
    "    with open(file_name, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    # extract each column as a list\n",
    "    Dev_log2_enrichment = []\n",
    "    Hk_log2_enrichment = []\n",
    "    Dev_log2_enrichment_scaled = []\n",
    "    Hk_log2_enrichment_scaled = []\n",
    "    Dev_log2_enrichment_quantile_normalized = []\n",
    "    Hk_log2_enrichment_quantile_normalized = []\n",
    "\n",
    "    for line in lines[1:]:\n",
    "        # split the line on whitespace and convert each value to a float\n",
    "        values = [float(val) for val in line.split()]\n",
    "\n",
    "        # append the values to the appropriate list\n",
    "        Dev_log2_enrichment.append(values[0])\n",
    "        Hk_log2_enrichment.append(values[1])\n",
    "        Dev_log2_enrichment_scaled.append(values[2])\n",
    "        Hk_log2_enrichment_scaled.append(values[3])\n",
    "        Dev_log2_enrichment_quantile_normalized.append(values[4])\n",
    "        Hk_log2_enrichment_quantile_normalized.append(values[5])\n",
    "\n",
    "    print(len(Dev_log2_enrichment))\n",
    "    print(len(Hk_log2_enrichment))\n",
    "    print(len(Dev_log2_enrichment_scaled))\n",
    "    print(len(Hk_log2_enrichment_scaled))\n",
    "    print(len(Dev_log2_enrichment_quantile_normalized))\n",
    "    print(len(Hk_log2_enrichment_quantile_normalized))\n",
    "\n",
    "\n",
    "    return Dev_log2_enrichment, Hk_log2_enrichment\n",
    "\n",
    "\n",
    "def parse_all(file_name, dev_values, hk_values):\n",
    "    \"\"\"\n",
    "    Converts a FASTA file to a dictionary.\n",
    "    :param file_name: The name of the input FASTA file.\n",
    "    :type file_name: str\n",
    "    :return: A dictionary of each key with a list of two elements: \n",
    "        the key is the original nucleotide sequence\n",
    "        the first element of the list contains the one-hot-encoding of the sequence (reference one_hot_encoding function above)\n",
    "        and the second element of the list contains the corresponding class names/enhancer identifier (0 or 1)\n",
    "    :rtype: dict\n",
    "    :author: Sydney Ballard\n",
    "    :acknowledgements: Adapted from parse_fasta_file, which Kush Gulati wrote for previous assignments\n",
    "    \"\"\"\n",
    "    # Initialize dictionary \n",
    "    # {seq: [one-hot-encoded sequence, Dev, Hk]}\n",
    "    sequence_data = {} # dict will hold 0/1 for class and sequence\n",
    "\n",
    "    sequences = []\n",
    "    one_hot_encodings = []\n",
    "\n",
    "    with open(file_name) as file:\n",
    "        seq = \"\"\n",
    "\n",
    "        new_line_count = 0\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if line.startswith(\">\"):\n",
    "                new_line_count += 1\n",
    "                if seq != \"\":\n",
    "                    sequences.append(seq)\n",
    "                    one_hot_encodings.append(one_hot_encoding(seq.strip(\"N\")))\n",
    "                seq = \"\"\n",
    "            else:\n",
    "                seq += line\n",
    "        sequences.append(seq)\n",
    "        one_hot_encodings.append(one_hot_encoding(seq.strip(\"N\")))\n",
    "\n",
    "\n",
    "    # for idx in range(len(sequences)):\n",
    "    #     sequence_data[sequences[idx]] = [one_hot_encodings[idx],\n",
    "    #                                      dev_values[idx],\n",
    "    #                                      hk_values[idx]]\n",
    "\n",
    "    # return sequence_data\n",
    "    return [sequences, one_hot_encodings, dev_values, hk_values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "285552\n",
      "285552\n",
      "285552\n",
      "285552\n",
      "285552\n",
      "285552\n"
     ]
    }
   ],
   "source": [
    "dev, hk = parse_sequence_activity_mpra(\"/Users/sydneyballard/Desktop/Desktop - Sydney’s MacBook Pro/CS 561/cs561 repository COLLABORATIVE W KUSH/CS561/Assignment3/MPRA/Sequences_activity_Train.txt\")\n",
    "metadata = parse_all(\"/Users/sydneyballard/Desktop/Desktop - Sydney’s MacBook Pro/CS 561/cs561 repository COLLABORATIVE W KUSH/CS561/Assignment3/MPRA/Sequences_Train.fa\", dev, hk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-11 23:29:37.662533: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Failed to find data adapter that can handle input: (<class 'list'> containing values of types {'(<class \\'list\\'> containing values of types {\\'(<class \\\\\\'list\\\\\\'> containing values of types {\"<class \\\\\\'int\\\\\\'>\"})\\'})'}), (<class 'dict'> containing {\"<class 'str'>\"} keys and {'(<class \\'list\\'> containing values of types {\"<class \\'float\\'>\"})'} values)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m model\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39madam\u001b[39m\u001b[39m'\u001b[39m, loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmean_squared_error\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     16\u001b[0m \u001b[39m# fit the model to the data\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m model\u001b[39m.\u001b[39;49mfit(sequences, {\u001b[39m'\u001b[39;49m\u001b[39mdev_output\u001b[39;49m\u001b[39m'\u001b[39;49m: dev_activation, \u001b[39m'\u001b[39;49m\u001b[39mhk_output\u001b[39;49m\u001b[39m'\u001b[39;49m: hk_activation}, epochs\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m, verbose\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n\u001b[1;32m     19\u001b[0m \u001b[39m# evaluate the model on the Dev data\u001b[39;00m\n\u001b[1;32m     20\u001b[0m dev_loss \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mevaluate(sequences, dev_activation, verbose\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.10/site-packages/keras/engine/data_adapter.py:1083\u001b[0m, in \u001b[0;36mselect_data_adapter\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m   1080\u001b[0m adapter_cls \u001b[39m=\u001b[39m [\u001b[39mcls\u001b[39m \u001b[39mfor\u001b[39;00m \u001b[39mcls\u001b[39m \u001b[39min\u001b[39;00m ALL_ADAPTER_CLS \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mcan_handle(x, y)]\n\u001b[1;32m   1081\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m adapter_cls:\n\u001b[1;32m   1082\u001b[0m     \u001b[39m# TODO(scottzhu): This should be a less implementation-specific error.\u001b[39;00m\n\u001b[0;32m-> 1083\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1084\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFailed to find data adapter that can handle \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1085\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39minput: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(_type_name(x), _type_name(y))\n\u001b[1;32m   1086\u001b[0m     )\n\u001b[1;32m   1087\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mlen\u001b[39m(adapter_cls) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   1088\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1089\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mData adapters should be mutually exclusive for \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1090\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mhandling inputs. Found multiple adapters \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m to handle \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1091\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39minput: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(adapter_cls, _type_name(x), _type_name(y))\n\u001b[1;32m   1092\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Failed to find data adapter that can handle input: (<class 'list'> containing values of types {'(<class \\'list\\'> containing values of types {\\'(<class \\\\\\'list\\\\\\'> containing values of types {\"<class \\\\\\'int\\\\\\'>\"})\\'})'}), (<class 'dict'> containing {\"<class 'str'>\"} keys and {'(<class \\'list\\'> containing values of types {\"<class \\'float\\'>\"})'} values)"
     ]
    }
   ],
   "source": [
    "# extract the input sequences and the output values\n",
    "sequences = metadata[1]\n",
    "dev_activation = metadata[2]\n",
    "hk_activation = metadata[3]\n",
    "\n",
    "# define the neural network architecture\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation='relu', input_shape=(4,)),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# fit the model to the data\n",
    "model.fit(sequences, {'dev_output': dev_activation, 'hk_output': hk_activation}, epochs=100, verbose=0)\n",
    "\n",
    "# evaluate the model on the Dev data\n",
    "dev_loss = model.evaluate(sequences, dev_activation, verbose=0)\n",
    "print('Dev MSE:', dev_loss)\n",
    "\n",
    "# evaluate the model on the Hk data\n",
    "hk_loss = model.evaluate(sequences, hk_activation, verbose=0)\n",
    "print('Hk MSE:', hk_loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f4eb27cb8195d1d94436f2b166ad1aeefaab8cfd8978b527a6f30e8ae31e1f9c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
